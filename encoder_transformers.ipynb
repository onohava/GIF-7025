{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "118512c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Init ###\n",
    "\n",
    "# Packages\n",
    "from typing import Callable, Any, Dict, List\n",
    "from functools import reduce\n",
    "\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "import evaluate\n",
    "\n",
    "import os\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Constants\n",
    "earthquake_prompt_features = [\"DATE\", \"PLACE\", \"LATITUDE\", \"LONGITUDE\", \"DEPTH\"]\n",
    "earthquake_prompt_template = \"on DATE utc, an earthquake struck at PLACE. the epicenter was located at latitude LATITUDE, longitude LONGITUDE, with a depth of DEPTH km beneath the earth's surface.\"\n",
    "\n",
    "train_test_split = 0.6\n",
    "eval_test_split = 0.5\n",
    "\n",
    "logging_steps = 500\n",
    "\n",
    "save_strategy = \"epoch\"\n",
    "save_steps = 2000\n",
    "save_total_limit = 3\n",
    "\n",
    "num_train_epochs = 3\n",
    "per_device_train_batch_size = 8\n",
    "per_device_eval_batch_size = per_device_train_batch_size\n",
    "\n",
    "shuffle_seed = 42\n",
    "\n",
    "num_proc = os.cpu_count()\n",
    "\n",
    "# Models paths\n",
    "models_paths = []\n",
    "\n",
    "models_paths.append(\"distilbert/distilbert-base-uncased\") # Distilbert-base-uncased: 67M params\n",
    "# models_paths.append(\"FacebookAI/roberta-base\") # Roberta-base: 110M params\n",
    "# models_paths.append(\"google-bert/bert-base-uncased\") # Bert-base-uncased: 110M params\n",
    "# models_paths.append(\"google-bert/bert-large-uncased\") # Bert-large-uncased: 340M params\n",
    "\n",
    "# Datasets paths\n",
    "datasets_paths = []\n",
    "\n",
    "datasets_paths.append(\"Datasets/Earthquakes-180d-filtered.csv\") # Earthquakes-180d Dataset\n",
    "# datasets_paths.append(\"Datasets/Earthquakes-1990-2023-filtered.csv\") # Earthquakes-1990-2023 Dataset\n",
    "\n",
    "datasets_prompts_paths = {dataset_path: dataset_path.replace(\"filtered\", \"prompts\") for dataset_path in datasets_paths}\n",
    "\n",
    "datasets_prompts_tokenized_paths = {dataset_path: {model_path: datasets_prompts_paths[dataset_path].replace(\".csv\", f\"-{model_path.lower()}-tokenized.parquet\")\n",
    "                                    for model_path in models_paths} for dataset_path in datasets_paths}\n",
    "\n",
    "datasets_prompts_tokenized_subsets_sizes = {}\n",
    "\n",
    "datasets_prompts_tokenized_subsets_sizes[datasets_paths[0]] = {\"18K\": 18000}\n",
    "# datasets_prompts_tokenized_subsets_sizes[datasets_paths[1]] = {\"1M\": int(1e6), \"2M\": int(2e6), \"3M\": int(3e6)}\n",
    "# datasets_prompts_tokenized_subsets_sizes[datasets_paths[0]] = {\"1M\": int(1e6)}\n",
    "\n",
    "datasets_prompts_tokenized_subsets_paths = {dataset_path: {model_path: {dataset_prompts_tokenized_subset_name:\n",
    "                                   datasets_prompts_tokenized_paths[dataset_path][model_path].replace(\".parquet\", \"-\" + dataset_prompts_tokenized_subset_name + \".parquet\")\n",
    "                                   for dataset_prompts_tokenized_subset_name in datasets_prompts_tokenized_subsets_sizes[dataset_path].keys()}\n",
    "                                   for model_path in models_paths} for dataset_path in datasets_paths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23f54aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Methods ###\n",
    "\n",
    "def load_dataset_from_file(dataset_path: str):\n",
    "    return load_dataset(dataset_path.split(\".\")[-1], data_files = dataset_path)[\"train\"]\n",
    "\n",
    "def create_dataset(dataset, dataset_path: str, create_dataset: Callable, create_dataset_params: Dict[str, Any],\n",
    "                   load_dataset: bool = True, save_dataset: bool = True):\n",
    "    print(f\"Start of creation of dataset ({dataset_path})\")\n",
    "    \n",
    "    # Load dataset\n",
    "    if load_dataset: dataset = load_dataset_from_file(dataset)\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = create_dataset(dataset, **create_dataset_params)\n",
    "\n",
    "    # Save dataset\n",
    "    if save_dataset: dataset.to_csv(dataset_path)\n",
    "\n",
    "    print(f\"End of creation of dataset ({dataset_path})\")\n",
    "\n",
    "    # Print dataset\n",
    "    # print_dataset(f\"Dataset ({dataset_path.replace(\".csv\", \"\")})\", dataset)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def create_prompts_dataset(dataset, prompt_template: str, prompt_features: List[str], target_feature: str):\n",
    "    \n",
    "    # Packages\n",
    "    from functools import reduce\n",
    "\n",
    "    remove_features = dataset.column_names\n",
    "\n",
    "    def create_prompt(instance):\n",
    "        features = {prompt_feature: str(instance[prompt_feature.lower()]).lower() for prompt_feature in prompt_features}\n",
    "        prompt = reduce(lambda prompt, feature: prompt.replace(*feature, 1), features.items(), prompt_template)\n",
    "        \n",
    "        return {\"prompt\": prompt, \"labels\": instance[target_feature]}\n",
    "    \n",
    "    dataset_prompts = dataset.map(create_prompt, num_proc = num_proc)\n",
    "    dataset_prompts = dataset_prompts.remove_columns(remove_features)\n",
    "\n",
    "    return dataset_prompts\n",
    "\n",
    "    # for instance in dataset:\n",
    "    #     features = {prompt_feature: str(instance[prompt_feature.lower()]).lower() for prompt_feature in prompt_features}\n",
    "    #     prompt = reduce(lambda prompt, feature: prompt.replace(*feature, 1), features.items(), prompt_template)\n",
    "    #     dataset_prompts.append({\"prompt\": prompt, \"labels\": instance[target_feature]})\n",
    "    # return Dataset.from_list(dataset_prompts)\n",
    "\n",
    "def create_train_eval_test_datasets(dataset):\n",
    "    train_test_dataset = dataset.train_test_split(test_size = 1 - train_test_split, seed = shuffle_seed)\n",
    "    eval_test_dataset = train_test_dataset[\"test\"].train_test_split(test_size = 1 - eval_test_split, seed = shuffle_seed)\n",
    "    return DatasetDict({\"train\": train_test_dataset[\"train\"], \"eval\": eval_test_dataset[\"train\"], \"test\": eval_test_dataset[\"test\"]})\n",
    "\n",
    "def create_tokenized_dataset(dataset, tokenizer):\n",
    "    return dataset.map(lambda instance: tokenizer(instance[\"prompt\"], padding = \"max_length\", truncation = True), batched = True, num_proc = num_proc)\n",
    "\n",
    "def create_subset(dataset, subset_size: int):\n",
    "    # Sample dataset\n",
    "    #subset = dataset.sample(n = (subset_size if subset_size <= len(dataset) else len(dataset)))\n",
    "    subset = dataset.shuffle(seed = shuffle_seed).select(range(subset_size if subset_size <= len(dataset) else len(dataset)))\n",
    "\n",
    "    # Reset index\n",
    "    # subset.reset_index(drop = True, inplace = True)\n",
    "\n",
    "    return subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "83b8a22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of creation of dataset (Datasets/Earthquakes-180d-prompts.csv)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ecb857a9f024d3b8654cc596b9949a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/17976 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82eeae4e40384de6ae4541c5fbf7e596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/18 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of creation of dataset (Datasets/Earthquakes-180d-prompts.csv)\n"
     ]
    }
   ],
   "source": [
    "### Create prompts datasets ###\n",
    "\n",
    "datasets_prompts = {dataset_path: create_dataset(dataset_path, datasets_prompts_paths[dataset_path], create_prompts_dataset,\n",
    "                    {\"prompt_template\": earthquake_prompt_template, \"prompt_features\": earthquake_prompt_features, \"target_feature\": \"magnitude\"})\n",
    "                    for dataset_path in datasets_paths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364ee824",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load prompts datasets ###\n",
    "\n",
    "datasets_prompts = {dataset_path: load_dataset(datasets_prompts_paths[dataset_path]) for dataset_path in datasets_paths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf75ce83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### (distilbert/distilbert-base-uncased) Model Configuration ###\n",
      "DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert/distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.45.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Load models ###\n",
    "\n",
    "models = {}\n",
    "models_tokenizers = {}\n",
    "\n",
    "for model_path in models_paths:\n",
    "    models[model_path] = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels = 1, device_map = {\"\": 0})\n",
    "    models_tokenizers[model_path] = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "    models_tokenizers[model_path].max_length = models[model_path].config.max_position_embeddings\n",
    "\n",
    "    print(f\"### ({model_path}) Model Configuration ###\")\n",
    "    print(models[model_path].config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46e87677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of creation of dataset (Datasets/Earthquakes-180d-prompts-distilbert/distilbert-base-uncased-tokenized.parquet)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d01e0744ceb425b8518c322f88c5cfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/17976 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2343f638a8c4787a27f3098f7266591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/18 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of creation of dataset (Datasets/Earthquakes-180d-prompts-distilbert/distilbert-base-uncased-tokenized.parquet)\n"
     ]
    }
   ],
   "source": [
    "### Create prompts tokenized datasets ###\n",
    "\n",
    "datasets_prompts_tokenized = {dataset_path: {model_path: create_dataset(datasets_prompts[dataset_path],\n",
    "                              datasets_prompts_tokenized_paths[dataset_path][model_path], create_tokenized_dataset,\n",
    "                              {\"tokenizer\": models_tokenizers[model_path]}, False, True)\n",
    "                              for model_path in models_paths} for dataset_path in datasets_paths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680a2761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca918142d9a74727b0c6c0177f409fa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/3125 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "8745124887"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Load prompts tokenized datasets ###\n",
    "\n",
    "# datasets_prompts_tokenized = {dataset_path: {model_path: load_dataset(datasets_prompts_tokenized_paths[dataset_path][model_path])\n",
    "#                               for model_path in models_paths} for dataset_path in datasets_paths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d37be3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of creation of dataset (Datasets/Earthquakes-180d-prompts-distilbert/distilbert-base-uncased-tokenized-18K.parquet)\n",
      "End of creation of dataset (Datasets/Earthquakes-180d-prompts-distilbert/distilbert-base-uncased-tokenized-18K.parquet)\n"
     ]
    }
   ],
   "source": [
    "### Create prompts tokenized datasets subsets ###\n",
    "\n",
    "datasets_prompts_tokenized_subsets = {dataset_path: {model_path: {dataset_prompts_tokenized_subset_name: create_dataset(datasets_prompts_tokenized[dataset_path][model_path],\n",
    "                             datasets_prompts_tokenized_subsets_paths[dataset_path][model_path][dataset_prompts_tokenized_subset_name],\n",
    "                             create_subset, {\"subset_size\": dataset_prompts_tokenized_subset_size}, False, False)\n",
    "                             for (dataset_prompts_tokenized_subset_name, dataset_prompts_tokenized_subset_size) in datasets_prompts_tokenized_subsets_sizes[dataset_path].items()}\n",
    "                             for model_path in models_paths} for dataset_path in datasets_paths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431c9089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'distilbert/distilbert-base-uncased': {'18K': Dataset({\n",
      "    features: ['prompt', 'magnitude', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 17976\n",
      "})}, 'FacebookAI/roberta-base': {'18K': Dataset({\n",
      "    features: ['prompt', 'magnitude', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 17976\n",
      "})}, 'google-bert/bert-base-uncased': {'18K': Dataset({\n",
      "    features: ['prompt', 'magnitude', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 17976\n",
      "})}, 'google-bert/bert-large-uncased': {'18K': Dataset({\n",
      "    features: ['prompt', 'magnitude', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 17976\n",
      "})}}\n",
      "{'distilbert/distilbert-base-uncased': {'18K': 'Datasets/Earthquakes-180d-prompts-distilbert/distilbert-base-uncased-tokenized-18K.csv'}, 'FacebookAI/roberta-base': {'18K': 'Datasets/Earthquakes-180d-prompts-facebookai/roberta-base-tokenized-18K.csv'}, 'google-bert/bert-base-uncased': {'18K': 'Datasets/Earthquakes-180d-prompts-google-bert/bert-base-uncased-tokenized-18K.csv'}, 'google-bert/bert-large-uncased': {'18K': 'Datasets/Earthquakes-180d-prompts-google-bert/bert-large-uncased-tokenized-18K.csv'}}\n"
     ]
    }
   ],
   "source": [
    "### Load prompts tokenized datasets subsets ###\n",
    "\n",
    "# datasets_prompts_tokenized_subsets = {dataset_path: {model_path: {dataset_prompts_tokenize_subset_name:\n",
    "#                                       load_dataset(datasets_prompts_tokenized_subsets_paths[dataset_path][model_path][dataset_prompts_tokenize_subset_name])\n",
    "#                                       for dataset_prompts_tokenize_subset_name in datasets_prompts_tokenized_subsets_sizes.keys()}\n",
    "#                                       for model_path in models_paths} for dataset_path in datasets_paths}\n",
    "\n",
    "# print(datasets_prompts_tokenized_subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d7a3bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of creation of dataset (Datasets/Earthquakes-180d-prompts-distilbert/distilbert-base-uncased-tokenized-18K.parquet)\n",
      "End of creation of dataset (Datasets/Earthquakes-180d-prompts-distilbert/distilbert-base-uncased-tokenized-18K.parquet)\n"
     ]
    }
   ],
   "source": [
    "### Create train, eval and test datasets ###\n",
    "\n",
    "datasets_prompts_tokenized_subsets_split = {dataset_path: {model_path: {dataset_prompts_tokenize_subset_name:\n",
    "                                            create_dataset(datasets_prompts_tokenized_subsets[dataset_path][model_path][dataset_prompts_tokenize_subset_name],\n",
    "                                            datasets_prompts_tokenized_subsets_paths[dataset_path][model_path][dataset_prompts_tokenize_subset_name],\n",
    "                                            create_train_eval_test_datasets, {}, False, False)\n",
    "                                            for dataset_prompts_tokenize_subset_name in datasets_prompts_tokenized_subsets_sizes[dataset_path].keys()}\n",
    "                                            for model_path in models_paths} for dataset_path in datasets_paths}\n",
    "\n",
    "# print(datasets_prompts_tokenized_subsets_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f8de6791",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Models training methods ###\n",
    "\n",
    "mse = evaluate.load(\"mse\")\n",
    "\n",
    "def compute_mse(eval_prediction):\n",
    "    logits, labels = eval_prediction\n",
    "    if isinstance(logits, tuple): logits = logits[0]\n",
    "    return mse.compute(predictions = logits, references = labels)\n",
    "\n",
    "def train_model(model, model_name: str, tokenized_dataset):\n",
    "    Trainer(\n",
    "        model = model,\n",
    "        args = TrainingArguments(\n",
    "            output_dir = \"Models/\" + model_name,\n",
    "            eval_strategy = \"steps\",\n",
    "            logging_steps = logging_steps,\n",
    "            save_strategy = save_strategy,\n",
    "            save_steps = save_steps,\n",
    "            save_total_limit = save_total_limit,\n",
    "            per_device_train_batch_size = per_device_train_batch_size,\n",
    "            per_device_eval_batch_size = per_device_eval_batch_size,\n",
    "            auto_find_batch_size = True,\n",
    "            num_train_epochs = num_train_epochs,\n",
    "            seed = shuffle_seed,\n",
    "            data_seed = shuffle_seed),\n",
    "        train_dataset = tokenized_dataset[\"train\"],\n",
    "        eval_dataset = tokenized_dataset[\"eval\"],\n",
    "        compute_metrics = compute_mse,\n",
    "    ).train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ac4a9376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Training model (distilbert/distilbert-base-uncased/Earthquakes-180d-prompts-distilbert/distilbert-base-uncased-tokenized-18K) ###\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01b55adb738e4126a1f28a60aee3c042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4047 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4883, 'grad_norm': 4.4373602867126465, 'learning_rate': 4.382258463059056e-05, 'epoch': 0.37}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fccba8d1e9b4d5ea079ec55f789a788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/450 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3410135805606842, 'eval_mse': 0.3410135798185388, 'eval_runtime': 98.1031, 'eval_samples_per_second': 36.645, 'eval_steps_per_second': 4.587, 'epoch': 0.37}\n",
      "{'loss': 0.3226, 'grad_norm': 4.988509178161621, 'learning_rate': 3.7645169261181124e-05, 'epoch': 0.74}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e5b33747020438691191b2573a9fd09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/450 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.30101853609085083, 'eval_mse': 0.3010185140652373, 'eval_runtime': 118.857, 'eval_samples_per_second': 30.246, 'eval_steps_per_second': 3.786, 'epoch': 0.74}\n",
      "{'loss': 0.2792, 'grad_norm': 3.1520936489105225, 'learning_rate': 3.1467753891771684e-05, 'epoch': 1.11}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a6ba5d8d3ed4c5fae1b9f4263bb044d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/450 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2462080419063568, 'eval_mse': 0.24620805719349464, 'eval_runtime': 123.3587, 'eval_samples_per_second': 29.143, 'eval_steps_per_second': 3.648, 'epoch': 1.11}\n",
      "{'loss': 0.2508, 'grad_norm': 12.74351692199707, 'learning_rate': 2.5290338522362245e-05, 'epoch': 1.48}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df2b4ea60eff4bf0b63befbd2a551342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/450 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.24487605690956116, 'eval_mse': 0.24487604719475026, 'eval_runtime': 123.7295, 'eval_samples_per_second': 29.055, 'eval_steps_per_second': 3.637, 'epoch': 1.48}\n",
      "{'loss': 0.2388, 'grad_norm': 8.427264213562012, 'learning_rate': 1.9112923152952806e-05, 'epoch': 1.85}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "938399d4ea6e4ba5a7b8105eec309e14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/450 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.21749182045459747, 'eval_mse': 0.21749181982826066, 'eval_runtime': 118.6388, 'eval_samples_per_second': 30.302, 'eval_steps_per_second': 3.793, 'epoch': 1.85}\n",
      "{'loss': 0.22, 'grad_norm': 2.6051442623138428, 'learning_rate': 1.2935507783543367e-05, 'epoch': 2.22}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f0d62b0f2534fbaae10cb670a2a54bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/450 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2152383178472519, 'eval_mse': 0.21523830849211972, 'eval_runtime': 114.7588, 'eval_samples_per_second': 31.327, 'eval_steps_per_second': 3.921, 'epoch': 2.22}\n",
      "{'loss': 0.2164, 'grad_norm': 2.0292117595672607, 'learning_rate': 6.7580924141339264e-06, 'epoch': 2.59}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec3af5e1d8d44edc8375123d45367b75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/450 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.21650002896785736, 'eval_mse': 0.21650005185066418, 'eval_runtime': 113.6435, 'eval_samples_per_second': 31.634, 'eval_steps_per_second': 3.96, 'epoch': 2.59}\n",
      "{'loss': 0.2008, 'grad_norm': 1.9443855285644531, 'learning_rate': 5.806770447244872e-07, 'epoch': 2.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ebdfe4099a248888581d3401aa8a6a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/450 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.20560233294963837, 'eval_mse': 0.20560233078109713, 'eval_runtime': 123.1276, 'eval_samples_per_second': 29.197, 'eval_steps_per_second': 3.655, 'epoch': 2.97}\n",
      "{'train_runtime': 3450.3417, 'train_samples_per_second': 9.377, 'train_steps_per_second': 1.173, 'train_loss': 0.27654232541690615, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "### Train models ###\n",
    "\n",
    "for dataset_path in datasets_paths:\n",
    "    for model_path in models_paths:\n",
    "        for (dataset_prompts_tokenized_subset_split_name, dataset_prompts_tokenized_subset_split) in datasets_prompts_tokenized_subsets_split[dataset_path][model_path].items():\n",
    "            model_name = model_path + \"/\" + \"\".join(datasets_prompts_tokenized_subsets_paths[dataset_path][model_path][dataset_prompts_tokenized_subset_split_name].split(\"/\", maxsplit = 1)[1].split(\".\")[:-1])\n",
    "            print(f\"### Training model ({model_name}) ###\")\n",
    "            train_model(models[model_path], model_name, dataset_prompts_tokenized_subset_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2428df1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(models[models_paths[0]].device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
