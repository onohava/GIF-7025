{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1667582",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Init ###\n",
    "\n",
    "# Packages\n",
    "from typing import List, Dict, Callable, Any\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Constants\n",
    "min_magnitude = 0\n",
    "max_magnitude = 10\n",
    "\n",
    "min_longitude = -180\n",
    "max_longitude = 180\n",
    "\n",
    "min_latitude = -90\n",
    "max_latitude = 90\n",
    "\n",
    "min_depth = 0\n",
    "\n",
    "shuffle_seed = 42\n",
    "\n",
    "# Datasets paths\n",
    "datasets_paths = []\n",
    "\n",
    "datasets_paths.append(\"Datasets/Earthquakes-180d.csv\") # Earthquakes-180d Dataset\n",
    "datasets_paths.append(\"Datasets/Earthquakes-1990-2023.csv\") # Earthquakes-1990-2023 Dataset\n",
    "\n",
    "remove_features = {}\n",
    "\n",
    "remove_features[datasets_paths[0]] = [\"id\", \"url\"]\n",
    "remove_features[datasets_paths[1]] = [\"time\", \"state\", \"status\", \"tsunami\", \"significance\", \"data_type\"]\n",
    "\n",
    "rename_features = {}\n",
    "\n",
    "rename_features[datasets_paths[0]] = {\"mag\": \"magnitude\", \"depth_km\": \"depth\", \"time_utc\": \"date\"}\n",
    "rename_features[datasets_paths[1]] = {\"magnitudo\": \"magnitude\"}\n",
    "\n",
    "datasets_filtered_paths = {dataset_path: dataset_path.replace(\".csv\", \"-filtered.csv\") for dataset_path in datasets_paths}\n",
    "\n",
    "datasets_filtered_subsets_sizes = {}\n",
    "\n",
    "datasets_filtered_subsets_sizes[datasets_paths[0]] = {\"18K\": 18000}\n",
    "datasets_filtered_subsets_sizes[datasets_paths[1]] = {\"1M\": int(1e6), \"2M\": int(2e6), \"3M\": int(3e6)}\n",
    "\n",
    "datasets_filtered_subsets_paths = {dataset_path: {dataset_filtered_subset_name:\n",
    "                                   datasets_filtered_paths[dataset_path].replace(\".csv\", \"-\" + dataset_filtered_subset_name + \".csv\")\n",
    "                                   for dataset_filtered_subset_name in datasets_filtered_subsets_sizes[dataset_path].keys()}\n",
    "                                   for dataset_path in datasets_paths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78056a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Methods ###\n",
    "\n",
    "def filter_dataset_feature(dataset, feature_name: str, min_value: float = float(\"-inf\"), max_value: float = float(\"inf\"), include_min_max: bool = True):\n",
    "    if include_min_max:\n",
    "        return dataset[(dataset[feature_name] >= min_value) & (dataset[feature_name] <= max_value)]\n",
    "    else:\n",
    "        return dataset[(dataset[feature_name] > min_value) & (dataset[feature_name] < max_value)]\n",
    "    \n",
    "def filter_dataset(dataset):\n",
    "    # Filter magnitude\n",
    "    dataset = filter_dataset_feature(dataset, \"magnitude\", min_magnitude, max_magnitude, False)\n",
    "\n",
    "    # Filter longitude\n",
    "    dataset = filter_dataset_feature(dataset, \"longitude\", min_longitude, max_longitude, True)\n",
    "\n",
    "    # Filter latitude\n",
    "    dataset = filter_dataset_feature(dataset, \"latitude\", min_latitude, max_latitude, True)\n",
    "\n",
    "    # Filter depth\n",
    "    dataset = filter_dataset_feature(dataset, \"depth\", min_depth, include_min_max = True)\n",
    "\n",
    "    # Substitutions\n",
    "    dataset[\"place\"] = dataset[\"place\"].map(lambda place: place.replace(\"CA\", \"California\"))\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def print_dataset(dataset_name: str, dataset):\n",
    "    print(f\"### {dataset_name} ###\")\n",
    "    print(dataset.info())\n",
    "    print(dataset.describe())\n",
    "\n",
    "def create_filtered_dataset(dataset, remove_features: List[str], rename_features: Dict[str, str],\n",
    "                            filter_dataset: Callable = filter_dataset):\n",
    "    # Rename features\n",
    "    dataset.rename(columns = rename_features, inplace = True)\n",
    "\n",
    "    # Filter dataset\n",
    "    dataset = filter_dataset(dataset)\n",
    "\n",
    "    # Features selection\n",
    "    dataset.drop(columns = remove_features, inplace = True)\n",
    "\n",
    "    # Drop duplicates\n",
    "    dataset.drop_duplicates(inplace = True)\n",
    "\n",
    "    # Reset index\n",
    "    dataset.reset_index(drop = True, inplace = True)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def create_dataset(dataset, dataset_path: str, create_dataset: Callable, create_dataset_params: Dict[str, Any],\n",
    "                   load_dataset: bool = True, save_dataset: bool = True):\n",
    "    print(f\"Start of creation of dataset ({dataset_path})\")\n",
    "    \n",
    "    # Load dataset\n",
    "    if load_dataset: dataset = pd.read_csv(dataset)\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = create_dataset(dataset, **create_dataset_params)\n",
    "\n",
    "    # Save dataset\n",
    "    if save_dataset: dataset.to_csv(dataset_path, index = False)\n",
    "\n",
    "    print(f\"End of creation of dataset ({dataset_path})\")\n",
    "\n",
    "    # Print dataset\n",
    "    print_dataset(f\"Dataset ({dataset_path.replace(\".csv\", \"\")})\", dataset)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def create_subset(dataset, subset_size: int):\n",
    "    # Sample dataset\n",
    "    subset = dataset.sample(n = (subset_size if subset_size <= len(dataset) else len(dataset)), random_state = shuffle_seed)\n",
    "\n",
    "    # Reset index\n",
    "    subset.reset_index(drop = True, inplace = True)\n",
    "\n",
    "    return subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1fb659",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Filters ###\n",
    "\n",
    "def filter_dataset2(dataset):\n",
    "    # Filter earthquakes\n",
    "    dataset = dataset[dataset[\"data_type\"] == \"earthquake\"]\n",
    "    \n",
    "    # Filter dataset\n",
    "    dataset = filter_dataset(dataset)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "filter_datasets = {}\n",
    "\n",
    "filter_datasets[datasets_paths[0]] = filter_dataset\n",
    "filter_datasets[datasets_paths[1]] = filter_dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a643fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create filtered datasets ###\n",
    "\n",
    "datasets_filtered = {dataset_path: create_dataset(dataset_path, datasets_filtered_paths[dataset_path], create_filtered_dataset,\n",
    "                     {\"remove_features\": remove_features[dataset_path], \"rename_features\": rename_features[dataset_path],\n",
    "                      \"filter_dataset\": filter_datasets[dataset_path]})\n",
    "                     for dataset_path in datasets_paths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a040cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load filtered datasets ###\n",
    "\n",
    "datasets_filtered = {dataset_path: pd.read_csv(datasets_filtered_paths[dataset_path]) for dataset_path in datasets_paths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b29a8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create filtered datasets subsets ###\n",
    "\n",
    "datasets_filtered_subsets = {dataset_path: {dataset_filtered_subset_name: create_dataset(datasets_filtered[dataset_path],\n",
    "                             datasets_filtered_subsets_paths[dataset_path][dataset_filtered_subset_name],\n",
    "                             create_subset, {\"subset_size\": dataset_filtered_subset_size}, False)\n",
    "                             for (dataset_filtered_subset_name, dataset_filtered_subset_size) in datasets_filtered_subsets_sizes[dataset_path].items()}\n",
    "                             for dataset_path in datasets_paths}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
