{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcbd9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Init ###\n",
    "\n",
    "# Packages\n",
    "from typing import Callable, Any, Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "import os, json\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import torch\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "# Constants\n",
    "earthquake_prompt_features = [\"PLACE\", \"DATE\", \"LATITUDE\", \"LONGITUDE\", \"DEPTH\"]\n",
    "earthquake_prompt_template = \"on DATE utc, an earthquake struck on PLACE. the epicenter was located at latitude LATITUDE, longitude LONGITUDE, with a depth of DEPTH km beneath the earth's surface.\"\n",
    "\n",
    "earthquake_place = \"earth\"\n",
    "earthquake_date = \"2023-01-01 00:00:00\"\n",
    "earthquake_depth = 50\n",
    "\n",
    "earthquake_prompt_template_heatmap = earthquake_prompt_template.replace(\"PLACE\", earthquake_place).replace(\"DATE\", earthquake_date).replace(\"DEPTH\", str(earthquake_depth))\n",
    "earthquake_prompt_features_heatmap = [\"LATITUDE\", \"LONGITUDE\"]\n",
    "\n",
    "step_size = 4\n",
    "\n",
    "min_longitude = -180\n",
    "max_longitude = 180\n",
    "\n",
    "min_latitude = -90\n",
    "max_latitude = 90\n",
    "\n",
    "longitude_grid, latitude_grid = np.meshgrid(\n",
    "    np.arange(min_longitude, max_longitude + step_size, step_size),\n",
    "    np.arange(min_latitude, max_latitude + step_size, step_size))\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "num_proc = os.cpu_count()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "shuffle_seed = 42\n",
    "\n",
    "# Datasets paths\n",
    "datasets_paths = []\n",
    "\n",
    "# datasets_paths.append(\"Datasets/Earthquakes-180d-filtered.csv\") # Earthquakes-180d Dataset\n",
    "datasets_paths.append(\"Datasets/Earthquakes-1990-2023-filtered.csv\") # Earthquakes-1990-2023 Dataset\n",
    "\n",
    "datasets_subsets_sizes = {}\n",
    "\n",
    "# datasets_subsets_sizes[datasets_paths[0]] = {\"18K\": (18000, 4047)}\n",
    "# datasets_subsets_sizes[datasets_paths[1]] = {\"1M\": (), \"2M\": (), \"3M\": ()}\n",
    "datasets_subsets_sizes[datasets_paths[0]] = {\"1M\": (int(1e6), 225000)}\n",
    "\n",
    "heatmap_dataset_path = \"Datasets/Heatmap.csv\"\n",
    "heatmap_dataset_prompts_path = heatmap_dataset_path.replace(\".csv\", \"-prompts.csv\")\n",
    "\n",
    "# Models paths\n",
    "models_paths = []\n",
    "\n",
    "models_paths.append(\"distilbert/distilbert-base-uncased\") # Distilbert-base-uncased: 67M params\n",
    "# models_paths.append(\"FacebookAI/roberta-base\") # Roberta-base: 110M params\n",
    "# models_paths.append(\"google-bert/bert-base-uncased\") # Bert-base-uncased: 110M params\n",
    "# models_paths.append(\"google-bert/bert-large-uncased\") # Bert-large-uncased: 340M params\n",
    "\n",
    "trained_models_paths = {dataset_path: {model_path: {dataset_subset_size_name:\n",
    "                        f\"{dataset_path.replace(\"Datasets/\", \"Models/\").replace(\"-filtered.csv\", \"-prompts-tokenized\")}/{model_path.lower()}-{dataset_subset_size_name}/checkpoint-{dataset_subset_size[1]}\"\n",
    "                        for (dataset_subset_size_name, dataset_subset_size) in datasets_subsets_sizes[dataset_path].items()}\n",
    "                        for model_path in models_paths} for dataset_path in datasets_paths}\n",
    "\n",
    "heatmap_dataset_prompts = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d273844b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Methods ###\n",
    "\n",
    "def load_dataset_from_file(dataset_path: str):\n",
    "    return load_dataset(dataset_path.split(\".\")[-1], data_files = dataset_path)[\"train\"]\n",
    "\n",
    "def load_model(model_path: str):\n",
    "    return AutoModelForSequenceClassification.from_pretrained(model_path, num_labels = 1)\n",
    "\n",
    "def create_dataset(dataset, dataset_path: str, create_dataset: Callable, create_dataset_params: Dict[str, Any],\n",
    "                   load_dataset: bool = True, save_dataset: bool = True):\n",
    "    print(f\"Start of creation of dataset ({dataset_path})\")\n",
    "    \n",
    "    # Load dataset\n",
    "    if load_dataset: dataset = load_dataset_from_file(dataset)\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = create_dataset(dataset, **create_dataset_params)\n",
    "\n",
    "    # Save dataset\n",
    "    if save_dataset:\n",
    "        if dataset_path.endswith(\".csv\"): dataset.to_csv(dataset_path)\n",
    "        elif dataset_path.endswith(\".parquet\"): dataset.to_parquet(dataset_path)\n",
    "\n",
    "    print(f\"End of creation of dataset ({dataset_path})\")\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def create_prompts_dataset(dataset, prompt_template: str, prompt_features: List[str]):\n",
    "    \n",
    "    # Packages\n",
    "    from functools import reduce\n",
    "\n",
    "    remove_features = dataset.column_names\n",
    "\n",
    "    def create_prompt(instance):\n",
    "        features = {prompt_feature: str(instance[prompt_feature.lower()]).lower() for prompt_feature in prompt_features}\n",
    "        prompt = reduce(lambda prompt, feature: prompt.replace(*feature, 1), features.items(), prompt_template)\n",
    "        \n",
    "        return {\"prompt\": prompt}\n",
    "    \n",
    "    dataset_prompts = dataset.map(create_prompt, num_proc = num_proc)\n",
    "    dataset_prompts = dataset_prompts.remove_columns(remove_features)\n",
    "\n",
    "    return dataset_prompts\n",
    "\n",
    "def create_tokenized_dataset(dataset, tokenizer):\n",
    "    return dataset.map(lambda instance: tokenizer(instance[\"prompt\"], padding = \"max_length\", truncation = True), batched = True, num_proc = num_proc)\n",
    "\n",
    "def create_subset(dataset, subset_size: int):\n",
    "    # Sample dataset\n",
    "    return dataset.shuffle(seed = shuffle_seed).select(range(subset_size if subset_size <= len(dataset) else len(dataset)))\n",
    "\n",
    "def print_model_error_visualization(dataset_path: str, model_path: str, dataset_subset_size_name: str, trained_model_path: str):\n",
    "    with open(trained_model_path + \"/trainer_state.json\") as trainer_state:\n",
    "        trainer_log_history = json.load(trainer_state)[\"log_history\"]\n",
    "\n",
    "    epochs_eval_mse = []\n",
    "    epochs_test_mse = []\n",
    "\n",
    "    eval_mse_errors = []\n",
    "    test_mse_errors = []\n",
    "\n",
    "    for eval in trainer_log_history:\n",
    "        if \"loss\" in eval:\n",
    "            epochs_eval_mse.append(eval[\"epoch\"])\n",
    "            eval_mse_errors.append(eval[\"loss\"])\n",
    "        else:\n",
    "            epochs_test_mse.append(eval[\"epoch\"])\n",
    "            test_mse_errors.append(eval[\"eval_loss\"])\n",
    "    \n",
    "    dataset_name = dataset_path.split(\"Datasets/\", maxsplit = 1)[1].replace(\".csv\", \"\")\n",
    "\n",
    "    plt.title(f\"### {model_path} ({dataset_name + \"-\" + dataset_subset_size_name}) ###\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"MSE error\")\n",
    "\n",
    "    plt.plot(epochs_eval_mse, eval_mse_errors, marker = \"o\", label = \"Train\")\n",
    "    plt.plot(epochs_test_mse, test_mse_errors, marker = \"o\", label = \"Test\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def print_model_heatmap_visualization(dataset_path: str, model_path: str, dataset_subset_size, trained_model, model_tokenizer):\n",
    "\n",
    "    def predict_magnitudes(batch):\n",
    "        inputs = model_tokenizer(batch[\"prompt\"], return_tensors = \"pt\", padding = True, truncation = True).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = trained_model.to(device)(**inputs)\n",
    "            magnitudes = outputs.logits.squeeze().tolist()\n",
    "\n",
    "        return {\"magnitude\": magnitudes}\n",
    "\n",
    "    magnitude_predictions = np.array(heatmap_dataset_prompts.map(predict_magnitudes, batched = True, batch_size = batch_size)[\"magnitude\"]).reshape(longitude_grid.shape)\n",
    "\n",
    "    dataset = create_dataset(dataset_path, dataset_path, create_subset, {\"subset_size\": dataset_subset_size[1][0]}, True, False)\n",
    "\n",
    "    fig = plt.figure(figsize = (25, 12.5))\n",
    "    ax = plt.axes(projection = ccrs.PlateCarree())\n",
    "\n",
    "    ax.add_feature(cfeature.COASTLINE, linewidth = 0.8)\n",
    "    ax.add_feature(cfeature.BORDERS, linewidth = 0.5)\n",
    "    ax.add_feature(cfeature.LAND, facecolor = \"lightgray\")\n",
    "\n",
    "    dataset_name = dataset_path.split(\"Datasets/\", maxsplit = 1)[1].replace(\".csv\", \"\")\n",
    "\n",
    "    plt.title(f\"### {model_path} ({dataset_name + \"-\" + dataset_subset_size[0]}) ###\", fontsize = 20)\n",
    "    plt.xlabel(\"Longitude\")\n",
    "    plt.ylabel(\"Latitude\")\n",
    "\n",
    "    img = ax.imshow(magnitude_predictions, origin = \"lower\", extent = [min_longitude, max_longitude, min_latitude, max_latitude], transform = ccrs.PlateCarree(), cmap = \"coolwarm\", alpha = 0.6)\n",
    "\n",
    "    cbar = plt.colorbar(img, ax = ax, orientation = \"vertical\")\n",
    "    cbar.ax.tick_params(labelsize = 15, direction = \"in\", pad = -30)\n",
    "\n",
    "    ax.scatter(dataset[\"longitude\"], dataset[\"latitude\"], c = dataset[\"magnitude\"], cmap = \"coolwarm\", transform = ccrs.PlateCarree(), edgecolors = \"black\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979a040c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load trained models ###\n",
    "\n",
    "models_tokenizers = {model_path: AutoTokenizer.from_pretrained(model_path) for model_path in models_paths}\n",
    "\n",
    "trained_models = {dataset_path: {model_path: {dataset_subset_size_name:\n",
    "                    load_model(trained_models_paths[dataset_path][model_path][dataset_subset_size_name])\n",
    "                    for dataset_subset_size_name in datasets_subsets_sizes[dataset_path].keys()}\n",
    "                    for model_path in models_paths} for dataset_path in datasets_paths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8f261c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MSE error models visualization ###\n",
    "\n",
    "for dataset_path in datasets_paths:\n",
    "    for model_path in models_paths:\n",
    "        for dataset_subset_size_name in datasets_subsets_sizes[dataset_path].keys():\n",
    "            print_model_error_visualization(dataset_path, model_path, dataset_subset_size_name, trained_models_paths[dataset_path][model_path][dataset_subset_size_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b10ff4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create heatmap dataset ###\n",
    "\n",
    "heatmap_dataset = Dataset.from_dict({\"longitude\": longitude_grid.ravel(), \"latitude\": latitude_grid.ravel()})\n",
    "\n",
    "heatmap_dataset.to_csv(heatmap_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03e491f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load heatmap dataset ###\n",
    "\n",
    "heatmap_dataset = load_dataset_from_file(heatmap_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ce6c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create heatmap prompts dataset ###\n",
    "\n",
    "heatmap_dataset_prompts_args = {\"prompt_template\": earthquake_prompt_template_heatmap, \"prompt_features\": earthquake_prompt_features_heatmap}\n",
    "\n",
    "heatmap_dataset_prompts = create_dataset(heatmap_dataset, heatmap_dataset_prompts_path, create_prompts_dataset, heatmap_dataset_prompts_args, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48ece5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load heatmap prompts dataset ###\n",
    "\n",
    "heatmap_dataset_prompts = load_dataset_from_file(heatmap_dataset_prompts_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217fe09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Heatmap earthquakes risk models visualization ###\n",
    "\n",
    "for dataset_path in datasets_paths:\n",
    "    for model_path in models_paths:\n",
    "        for dataset_subset_size in datasets_subsets_sizes[dataset_path].items():\n",
    "            print_model_heatmap_visualization(dataset_path, model_path, dataset_subset_size, trained_models[dataset_path][model_path][dataset_subset_size[0]], models_tokenizers[model_path])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
